{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bf75c9-84c1-4782-915a-51ade4ad3304",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Run the docker image and check if the Kafka server has any defined topics:\n",
    "    - start terminal and in jupyterlab directory run docker compose up\n",
    "    - in the additional window of the terminal, enter the command:\n",
    "        \n",
    "```bash\n",
    "    docker exec broker kafka-topics --list --bootstrap-server broker:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c626fa3-67ec-47ef-9975-dd9e6e956059",
   "metadata": {},
   "source": [
    "2. Add topic `streaming`\n",
    "```bash\n",
    "docker exec broker kafka-topics --bootstrap-server broker:9092 --create --topic streaming\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a496e-b313-4ac2-b611-514fd50001c5",
   "metadata": {},
   "source": [
    "3. check the list of topics again making sure you have the topic `streaming`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d12c1a-2461-44f0-9a2c-447a322cea09",
   "metadata": {},
   "source": [
    "4. Launch a new terminal on your computer and create common data for the new topic\n",
    "\n",
    "```bash\n",
    "docker exec --interactive --tty broker kafka-console-producer --bootstrap-server broker:9092 --topic streaming\n",
    "```\n",
    "\n",
    "5. To check if sending messages is working, launch another terminal window and enter the following command to execute the consumer:\n",
    "\n",
    "```bash\n",
    "docker exec --interactive --tty broker kafka-console-consumer --bootstrap-server broker:9092 --topic streaming --from-beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97c868-1a93-42a0-a18e-09abd12f9a02",
   "metadata": {},
   "source": [
    "Complete the script so that it generates the data:\n",
    "\n",
    "1. create a `message` variable, which will be a dictionary containing information of a single event (key: value):\n",
    "    - \"time\" : current time + timedelta(seconds=random.randint(-15, 0))\n",
    "    - \"id\" : randomly selected from lists [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    - \"value: random value between 0 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f2a0f7-3eb5-4fba-856e-1cf069a71a10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file stream.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SERVER = \"broker:9092\"\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[SERVER],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "        api_version=(2, 7, 0),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            t = datetime.now() + timedelta(seconds=random.randint(-15, 0))\n",
    "            \n",
    "            message = {\n",
    "                \"time\" : str(t),\n",
    "                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"]),\n",
    "                \"values\" : random.randint(0,100)\n",
    "            }\n",
    "            \n",
    "            \n",
    "            producer.send(\"streaming\", value=message)\n",
    "            sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20396465-a863-4ab1-80c1-653fae24f1b0",
   "metadata": {},
   "source": [
    "2.  in jupyterlab terminal run a `stream.py` file\n",
    "```bash\n",
    "python stream.py\n",
    "```\n",
    "check in the consumer window if the sent messages come to Kafka.\n",
    "\n",
    "The `kafka-python` library is responsible for running the kafka import\n",
    "which you can install with `pip install kafka-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1425b3e2-9b3d-4b94-b995-5f7df5e29118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting raw_app.py\n"
     ]
    }
   ],
   "source": [
    "%%file raw_app.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, IntegerType\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## create spark variable\n",
    "    #YOUR CODE HERE\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    \n",
    "    json_schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"values\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"broker:9092\")\n",
    "        .option(\"subscribe\", \"streaming\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    parsed = raw.select(\n",
    "        \"timestamp\", f.from_json(raw.value.cast(\"string\"), json_schema).alias(\"json\")\n",
    "    ).select(\n",
    "        f.col(\"timestamp\").alias(\"proc_time\"),\n",
    "        f.col(\"json\").getField(\"time\").alias(\"event_time\"),\n",
    "        f.col(\"json\").getField(\"id\").alias(\"id\"),\n",
    "        f.col(\"json\").getField(\"values\").alias(\"value\"),\n",
    "    )\n",
    "    \n",
    "    info = parsed.groupBy(\"id\").count()\n",
    "    \n",
    "    query = (\n",
    "        info.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e37818-e348-48c2-b168-866d00da3f12",
   "metadata": {},
   "source": [
    "run streaming analysis: \n",
    "```bash\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 raw_app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e770992-f4e3-4596-9279-ddc82df84bfe",
   "metadata": {},
   "source": [
    "Modify pragram `raw_app.py` by:\n",
    "```python\n",
    "    json_schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"value\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "```\n",
    "and parsed stream \n",
    "\n",
    "```python\n",
    "    parsed = raw.select(\n",
    "        \"timestamp\", f.from_json(raw.value.cast(\"string\"), json_schema).alias(\"json\")\n",
    "    ).select(\n",
    "        f.col(\"timestamp\").alias(\"proc_time\"),\n",
    "        f.col(\"json\").getField(\"time\").alias(\"event_time\"),\n",
    "        f.col(\"json\").getField(\"id\").alias(\"id\"),\n",
    "        f.col(\"json\").getField(\"value\").alias(\"value\"),\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc19e99-970f-49a5-8863-c9a03ab4dcc9",
   "metadata": {},
   "source": [
    "## count the number of events by group ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
