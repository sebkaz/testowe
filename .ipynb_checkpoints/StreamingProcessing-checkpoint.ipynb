{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744dc469-91ee-4901-8c6f-f5abd232cfd2",
   "metadata": {},
   "source": [
    "# Przetwarzanie danych strumieniowych\n",
    "\n",
    "1. Uruchom obraz dockera i sprawdź czy serwer Kafki posiada jakieś zdefiniowane topici:\n",
    "    - uruchom terminal i w katalogu jupyterlab uruchom docker compose up\n",
    "    - w dodatkowym oknie termianala wpisz polecenie:\n",
    "    ```bash\n",
    "        docker exec broker kafka-topics --list --bootstrap-server broker:9092\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00668246-0f77-4997-9a17-7cb2d12dd056",
   "metadata": {},
   "source": [
    "2. dodaj topic o nazwie `streaming`\n",
    "```bash\n",
    "docker exec broker kafka-topics --bootstrap-server broker:9092 --create --topic streaming\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612a460-0657-485b-a9f7-2e15b5632a2f",
   "metadata": {},
   "source": [
    "3. sprawdź listę tematów ponownie upewniając się, że posiadasz temat `streaming`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ed426-cf12-48b7-afcd-82a88d56afa9",
   "metadata": {},
   "source": [
    "4. Uruchom nowy terminal na swoim komputerze i utwórz producenta generującego dane do nowego topicu\n",
    "```bash\n",
    "docker exec --interactive --tty broker kafka-console-producer --bootstrap-server broker:9092 --topic streaming\n",
    "```\n",
    "\n",
    "Aby sprawdzić czy wysyłanie wiadomości działa uruchom kolejne okno terminala i wpisz następującą komendę realizującą consumenta: \n",
    "\n",
    "```bash\n",
    "docker exec --interactive --tty broker kafka-console-consumer --bootstrap-server broker:9092 --topic streaming --from-beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798a36f-6909-415f-877b-5004c5c624cf",
   "metadata": {},
   "source": [
    "## Uruchomienie kodu wysyłającego strumień automatycznie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e14cb-dd1a-4fec-9015-08670683353e",
   "metadata": {},
   "source": [
    "Uzupełnij skrypt tak by generował następujące dane: \n",
    "\n",
    "1. utwórz zmienną `message` która będzie słownikiem zawierającym informacje pojedynczego eventu (klucz: wartość): \n",
    "    - \"time\" : aktualny czas + timedelta(seconds=random.randint(-15, 0))\n",
    "    - \"id\" : wybierane losowo z listy [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    - \"value: losowa wartość z zakresu 0 do 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e8e038-4c02-4e2f-a9ef-dae6c2951a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file stream.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SERVER = \"broker:9092\"\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[SERVER],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "        api_version=(2, 7, 0),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ##### \n",
    "            #    YOUR CODE HERE\n",
    "\n",
    "\n",
    "            #####\n",
    "            producer.send(\"streaming\", value=message)\n",
    "            sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07229dc-b564-4e58-9136-0901f273f174",
   "metadata": {},
   "source": [
    "2.  w terminalu jupyterlab uruchom plik `stream.py`\n",
    "```bash\n",
    "python stream.py\n",
    "```\n",
    "\n",
    "sprawdz w oknie consumenta czy wysyłane wiadomości przychodzą do Kafki.\n",
    "\n",
    "Za uruchomienie importu kafka odpowiedzialna jest biblioteka `kafka-python`\n",
    "którą możesz zainstalować poleceniem `pip install kafka-python`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb725f-3075-42b0-9c39-d410894c4573",
   "metadata": {},
   "source": [
    "## KOD APACHE SPARK 1\n",
    "\n",
    "Przygotuj kod skryptu który pobierze informacje z przesyłanego strumienia danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf9a50c-0463-4313-af82-17a3ed3e559b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting raw_app.py\n"
     ]
    }
   ],
   "source": [
    "%%file raw_app.py\n",
    "\n",
    "## LOAD SPARK SESSION object\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, IntegerType\n",
    "###\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# from pyspark.sql.functions import from_json, col\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## create spark variable\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    \n",
    "    ## \n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    \n",
    "    json_schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"value\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ## load stream data from topic streaming\n",
    "    # topic subscription\n",
    "    \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"broker:9092\")\n",
    "        .option(\"subscribe\", \"streaming\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    #parsed = raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "    \n",
    "    parsed = raw.select(\n",
    "        \"timestamp\", f.from_json(raw.value.cast(\"string\"), json_schema).alias(\"json\")\n",
    "    ).select(\n",
    "        f.col(\"timestamp\").alias(\"proc_time\"),\n",
    "        f.col(\"json\").getField(\"time\").alias(\"event_time\"),\n",
    "        f.col(\"json\").getField(\"id\").alias(\"id\"),\n",
    "        f.col(\"json\").getField(\"value\").alias(\"value\"),\n",
    "    )\n",
    "    \n",
    "    info = parsed.groupBy(\"id\").count()\n",
    "    \n",
    "    # defining output as console with outputMode as append\n",
    "    query =  (\n",
    "        info.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .option(\"truncate\", \"false\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    query.awaitTermination()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350f4ec-8c47-4415-abbe-73a0d483e62e",
   "metadata": {},
   "source": [
    "uruchom pierwsze przetwarzanie strumienia: \n",
    "```bash\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 raw_app.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6518d0a-c874-412a-9800-1f41ee12ccfe",
   "metadata": {},
   "source": [
    "Zmodyfikuj pragram `raw_app.py` dodając schemat:\n",
    "```python\n",
    "    json_schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"value\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "```\n",
    "oraz sparsowany strumień \n",
    "\n",
    "```python\n",
    "    parsed = raw.select(\n",
    "        \"timestamp\", f.from_json(raw.value.cast(\"string\"), json_schema).alias(\"json\")\n",
    "    ).select(\n",
    "        f.col(\"timestamp\").alias(\"proc_time\"),\n",
    "        f.col(\"json\").getField(\"time\").alias(\"event_time\"),\n",
    "        f.col(\"json\").getField(\"id\").alias(\"id\"),\n",
    "        f.col(\"json\").getField(\"value\").alias(\"value\"),\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a74acd-1c6b-4c37-a3ed-778b5d4d8585",
   "metadata": {},
   "source": [
    "uruchom kod sprawdzając czy widzisz przychodzące eventy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47232d0-418e-4cc2-a286-6a5a67425d0d",
   "metadata": {},
   "source": [
    "## zlicz ilość eventów ze względu na grupę ID \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c8574-6a74-4467-bcff-0125faae398c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
